////  StoryPageViewModel.swift//  MemoirAI////  Created by ChatGPT on 2025-06-08.//  Re-engineered identity-locking & negative-prompt logic.//import Foundationimport SwiftUIimport CoreDataimport CoreImage.CIFilterBuiltins@MainActorclass StoryPageViewModel: ObservableObject {    enum PageItem {        case illustration(image: UIImage, caption: String)        case textPage(index: Int, total: Int, body: String)        case qrCode(id: UUID, url: URL)    }    @Published var isLoading      = false    @Published var errorMessage   : String?    @Published var images         : [UIImage] = []    @Published var progress       : Double    = 0    @Published var pageItems      : [PageItem] = []    @Published var subjectPhoto   : UIImage?    @Published var subjectPhotoID : String?    @Published var styleTile      : UIImage?    @Published var styleTileID    : String?    private   var subjectPhotoJPEG: Data?    @AppStorage("memoirPageCount")            private var pageCountSetting        = 2    @AppStorage("memoirArtStyle")             private var artStyleRaw             = ArtStyle.realistic.rawValue    @AppStorage("memoirCustomArtStyleText")   private var customArtStyleText      = ""    @AppStorage("memoirEthnicity")            private var ethnicity               = ""    @AppStorage("memoirGender")               private var gender                  = ""    @AppStorage("memoirOtherPersonalDetails") private var otherDetails            = ""    private var currentArtStyle : ArtStyle { ArtStyle(rawValue: artStyleRaw) ?? .realistic }    private var faceDescription : String?        // filled once per session    private let promptGen : PromptGenerator    private let imageCtx  : ImageContext    private let imageSvc  : OpenAIImageService    private let openAIKey : String    init() {        guard let key = Bundle.main.object(forInfoDictionaryKey: "OPENAI_API_KEY") as? String,              !key.isEmpty, !key.contains("YOUR_API") else {            fatalError("OPENAI_API_KEY missing or invalid")        }        openAIKey = key        promptGen = PromptGenerator(apiKey: key)        imageCtx  = ImageContext(apiKey: key)        imageSvc  = OpenAIImageService(apiKey: key)    }    func expectedPageCount() -> Int { pageCountSetting }    var  styleTilePublic: UIImage? { styleTile }    private func ensureSubjectPhotoIsRegistered() async {        guard subjectPhotoID == nil, let shot = subjectPhoto else { return }        do {            let (fid, jpeg) = try await imageCtx.createReference(from: shot)            subjectPhotoID   = fid            subjectPhotoJPEG = jpeg            print("✅ head-shot uploaded →", fid)        } catch {            print("🚫 head-shot upload failed:", error.localizedDescription)        }    }    // Ask Vision for a one-liner race / hair / skin descriptor    private func ensureFaceDescription() async {        guard faceDescription == nil,              let fid = subjectPhotoID else { return }        do {            faceDescription = try await imageCtx.faceDescriptor(                fileID: fid,                jpegData: subjectPhotoJPEG      // inline image avoids URL restrictions            )            print("✅ face descriptor →", faceDescription!)        } catch {            print("🚫 face descriptor failed:", error.localizedDescription)            faceDescription = nil        }    }    /// Returns the prefix + negative block that gets prepended to *every* photoreal prompt.    private func buildIdentityPrompt() -> String {        // 1️⃣ Natural language description from Vision if we have it        var identityBits: [String] = []        if let vision = faceDescription, !vision.isEmpty {            identityBits.append(vision)        }        // 2️⃣ Manual user fields as fallback / supplement        if !ethnicity.isEmpty { identityBits.append(ethnicity) }        if !gender.isEmpty    { identityBits.append(gender)    }        if !otherDetails.isEmpty {            identityBits.append(otherDetails)        }        guard !identityBits.isEmpty else { return "" }        // Positive directive        let positive = """        CHARACTERS: \(identityBits.joined(separator: ", ")). Faces must keep *all* those traits. \        Family and close friends default to the same race and similar skin-tone unless the script explicitly says otherwise.        """        // Negative guard – concise but powerful        let negative = """        NEGATIVE: mismatched race, pale Caucasian skin, light-blond hair, incorrect gender presentation, \        non-African-textured hair, interracial family (unless specified).        """        return positive + " " + negative + " "    }    func generateStorybook(forProfileID id: UUID) async {        isLoading = true        errorMessage = nil        progress = 0        images.removeAll()        pageItems.removeAll()        await ensureSubjectPhotoIsRegistered()        await ensureFaceDescription()        do {            let entries = try await fetchMemoryEntries(for: id)            let chosen  = await rankMemoriesWithLLM(entries, top: pageCountSetting)            guard !chosen.isEmpty else {                throw NSError(domain: "MemoirAI",                              code: 1,                              userInfo: [NSLocalizedDescriptionKey: "No memories chosen."])            }            let identityPrefix = buildIdentityPrompt()   // ← always include, no art-style check            var generated: [UIImage] = []            for (idx, entry) in chosen.enumerated() {                guard let entryID = entry.id else { continue }                let raw = entry.text?.trimmingCharacters(in: .whitespacesAndNewlines) ?? ""                guard !raw.isEmpty else { continue }                // Turn the memory into (1) an image prompt, (2) a caption                let content = try await promptGen.generatePrompts(                    from: raw,                    pageCount: 1,                    chosenArtStyle: currentArtStyle,                    customArtStyleDetails: customArtStyleText                ).first!                // Final prompt assembly                var promptToSend = identityPrefix + content.imagePromptText                if currentArtStyle == .realistic {                    promptToSend += """                     Camera pulled back, face partly turned away or softly out of focus so exact features \                     are not discernible. Or another method where the face isn’t perfectly clear.                     """                }                print("🖼️ FINAL PROMPT ►", promptToSend)                let img = try await imageSvc.generateImages(                    prompt: promptToSend,                    n: 1,                    size: "1792x1024"                ).first ?? UIImage()                generated.append(img)                // 1) illustration page                pageItems.append(.illustration(image: img,                                               caption: content.pageDisplayText))                // 2) paginated text pages                let chunks = raw.paginated()                for (i, chunk) in chunks.enumerated() {                    pageItems.append(.textPage(index: i + 1,                                               total: chunks.count,                                               body: chunk))                }                // 3) QR code                pageItems.append(.qrCode(                    id: entryID,                    url: URL(string: "memoirai://memory/\(entryID.uuidString)")!                ))                progress = Double(idx + 1) / Double(chosen.count)            }            images = generated        } catch {            errorMessage = error.localizedDescription            print("StoryPageViewModel ERROR:", error.localizedDescription)        }        isLoading = false    }    private func fetchMemoryEntries(for profileID: UUID) async throws -> [MemoryEntry] {        let ctx = PersistenceController.shared.container.viewContext        return try await ctx.perform {            let req: NSFetchRequest<MemoryEntry> = MemoryEntry.fetchRequest()            req.predicate = NSPredicate(format: "profileID == %@", profileID as CVarArg)            req.sortDescriptors = [NSSortDescriptor(keyPath: \MemoryEntry.createdAt, ascending: true)]            return try ctx.fetch(req)        }    }    private struct MemoryStub: Codable { let id: UUID; let summary: String }    private struct ChatMessage: Encodable { let role: String; let content: String }    private struct ChatCompletionRequest: Encodable {        let model: String; let messages: [ChatMessage]        let max_tokens: Int; let temperature: Double    }    private func rankMemoriesWithLLM(_ all: [MemoryEntry], top n: Int) async -> [MemoryEntry] {        guard n < all.count else { return all }        // Light-weight stubs for the LLM        let stubs = all.compactMap { mem -> MemoryStub? in            guard let id = mem.id,                  let txt = mem.text?.trimmingCharacters(in: .whitespacesAndNewlines),                  !txt.isEmpty else { return nil }            return MemoryStub(id: id,                              summary: txt.split(separator: " ").prefix(25).joined(separator: " "))        }        guard let stubJSON = try? JSONEncoder().encode(stubs),              let stubStr   = String(data: stubJSON, encoding: .utf8) else {            return Array(all.prefix(n))        }        let system = ChatMessage(role: "system",                                 content: "You are a memoir editor. Pick the \(n) most emotionally significant memories.")        let user   = ChatMessage(role: "user",                                 content: """                                 Return ONLY JSON { "top": ["uuid1","uuid2"] }.                                 Memories: \(stubStr)                                 """)        let req = ChatCompletionRequest(model: "gpt-4o-mini",                                        messages: [system, user],                                        max_tokens: 64,                                        temperature: 0)        var urlReq = URLRequest(url: URL(string: "https://api.openai.com/v1/chat/completions")!)        urlReq.httpMethod = "POST"        urlReq.httpBody   = try? JSONEncoder().encode(req)        urlReq.addValue("Bearer \(openAIKey)", forHTTPHeaderField: "Authorization")        urlReq.addValue("application/json", forHTTPHeaderField: "Content-Type")        do {            let (data, _) = try await URLSession.shared.data(for: urlReq)            guard                let root    = try? JSONSerialization.jsonObject(with: data) as? [String:Any],                let choices = root["choices"] as? [[String:Any]],                let msgDict = choices.first?["message"] as? [String:Any],                let content = msgDict["content"] as? String,                let idsDict = try? JSONDecoder().decode([String:[UUID]].self,                                                        from: Data(content.utf8)),                let ids     = idsDict["top"]            else { return Array(all.prefix(n)) }            return all.filter { $0.id.map(ids.contains) ?? false }        } catch {            print("LLM ranking failed:", error.localizedDescription)            return Array(all.prefix(n))        }    }}extension UIImage {    /// Shrinks the image if its longest side exceeds `maxSide`.    func resized(maxSide: CGFloat) -> UIImage {        let longest = max(size.width, size.height)        guard longest > maxSide else { return self }        let scale  = maxSide / longest        let newSz  = CGSize(width: size.width * scale, height: size.height * scale)        UIGraphicsBeginImageContextWithOptions(newSz, false, 0)        draw(in: CGRect(origin: .zero, size: newSz))        let out = UIGraphicsGetImageFromCurrentImageContext() ?? self        UIGraphicsEndImageContext()        return out    }    /// Produces a crisp QR code bitmap.    static func qrCode(from text: String, size: CGFloat = 300) -> UIImage {        let ctx = CIContext()        let f   = CIFilter.qrCodeGenerator()        f.message = Data(text.utf8)        guard let ci = f.outputImage else { return UIImage() }        let scaleX = size / ci.extent.size.width        let scaleY = size / ci.extent.size.height        let scaled = ci.transformed(by: .init(scaleX: scaleX, y: scaleY))        if let cg = ctx.createCGImage(scaled, from: scaled.extent) {            return UIImage(cgImage: cg)        }        return UIImage()    }}extension String {    func paginated(wordsPerPage: Int = 130) -> [String] {        let words = split { $0.isWhitespace }        guard words.count > wordsPerPage else { return [self] }        var pages: [String] = []        var i = 0        while i < words.count {            let j = min(i + wordsPerPage, words.count)            pages.append(words[i..<j].joined(separator: " "))            i += wordsPerPage        }        return pages    }}